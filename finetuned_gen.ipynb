{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from detoxify import Detoxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained('fine_tuned_model')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('fine_tuned_model')\n",
    "toxic_model = Detoxify(\"original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt_text):\n",
    "    \n",
    "    ## Do not generate text if input bio fails toxicity test\n",
    "    toxicity_rubric_input = toxic_model.predict(prompt_text)\n",
    "    if toxicity_rubric_input['severe_toxicity'] > .1 or toxicity_rubric_input['threat'] > .01:\n",
    "        return \"Your generated match cannot be shown due to harmful material in your bio. Please modify and try again.\"\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
    "    output = fine_tuned_model.generate(input_ids, min_length=200, max_length=500, num_return_sequences=1, temperature=.9)\n",
    "    decoded_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    toxicity_rubric_generated = toxic_model.predict(decoded_str)\n",
    "    \n",
    "    ## Do not return generated text if it fails toxicity test\n",
    "    if toxicity_rubric_generated['severe_toxicity'] > .1 or toxicity_rubric_generated['threat'] > .01:\n",
    "        return \"Your generated match cannot be shown. Please try again.\"\n",
    "    \n",
    "    ## Return generated bio if generated bio passes toxicity tests\n",
    "    return decoded_str\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
